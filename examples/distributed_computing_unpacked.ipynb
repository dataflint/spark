{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Computing Unpacked\n",
    "\n",
    "This notebook demonstrates various distributed computing patterns in Apache Spark, including partitioning strategies, aggregations, joins, and window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "from pyspark.sql.functions import col, concat, lit, rand, floor\n",
    "from pyspark.sql.functions import count as count_func\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Distributed Compute Examples\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.dataflint:spark_2.12:0.8.3\") \\\n",
    "    .config(\"spark.plugins\", \"io.dataflint.spark.SparkDataflintPlugin\") \\\n",
    "    .config(\"spark.ui.port\", \"11000\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(os.getenv('SALES_FILES_LOCATION'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy with Default Partitioning\n",
    "\n",
    "This example shows a basic groupBy operation with default partitioning behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription(\"GroupBy store and customer - default partitioning\")\n",
    "grouped_count = df.groupBy(\"ss_store_sk\", \"ss_customer_sk\").count()\n",
    "grouped_count.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy with Repartition\n",
    "\n",
    "This example demonstrates using repartition before groupBy to optimize data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription(\"GroupBy store and customer - with repartition\")\n",
    "grouped_count = df.repartition(\"ss_store_sk\", \"ss_customer_sk\") \\\n",
    "                  .groupBy(\"ss_store_sk\", \"ss_customer_sk\") \\\n",
    "                  .count()\n",
    "grouped_count.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy with Coalesce\n",
    "\n",
    "This example shows using coalesce to reduce the number of partitions before aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription(\"GroupBy store and customer - with coalesce(6)\")\n",
    "grouped_count = df.coalesce(6) \\\n",
    "                  .groupBy(\"ss_store_sk\", \"ss_customer_sk\") \\\n",
    "                  .count()\n",
    "grouped_count.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy with Salting\n",
    "\n",
    "This example demonstrates the salting technique to handle data skew by adding a random salt column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_salts = 8\n",
    "\n",
    "df_with_salt = df.withColumn(\n",
    "    \"salt\",\n",
    "    floor(rand(seed=42) * num_salts)\n",
    ")\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"GroupBy store and customer - with salting\")\n",
    "grouped_count_final = df_with_salt.groupBy(\"ss_store_sk\", \"ss_customer_sk\", \"salt\") \\\n",
    "            .count() \\\n",
    "            .groupBy(\"ss_store_sk\", \"ss_customer_sk\") \\\n",
    "            .count()\n",
    "grouped_count_final.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy with Join\n",
    "\n",
    "This example shows aggregating data separately and then joining the results with filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription(\"GroupBy with join - count by store and customer, filter > 3 or > 5\")\n",
    "store_counts = df.groupBy(\"ss_store_sk\") \\\n",
    "    .agg(count_func(\"*\").alias(\"store_count\")) \\\n",
    "    .filter(col(\"store_count\") > 3)\n",
    "\n",
    "customer_counts = df.groupBy(\"ss_customer_sk\") \\\n",
    "    .agg(count_func(\"*\").alias(\"customer_count\")) \\\n",
    "    .filter(col(\"customer_count\") > 5)\n",
    "\n",
    "join_result = df.select(\"ss_store_sk\", \"ss_customer_sk\") \\\n",
    "    .join(store_counts, \"ss_store_sk\", \"left\") \\\n",
    "    .join(customer_counts, \"ss_customer_sk\", \"left\") \\\n",
    "    .filter(col(\"store_count\").isNotNull() | col(\"customer_count\").isNotNull()) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "This example demonstrates using window functions to compute counts partitioned by different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription(\"Window functions - count by store and customer, filter > 3\")\n",
    "df_with_store_count = df.select(\"ss_store_sk\", \"ss_customer_sk\") \\\n",
    "    .withColumn(\"store_count\", count_func(\"*\").over(Window.partitionBy(\"ss_store_sk\"))) \\\n",
    "    .withColumn(\"customer_count\", count_func(\"*\").over(Window.partitionBy(\"ss_customer_sk\"))) \\\n",
    "    .filter((col(\"store_count\") > 3) | (col(\"customer_count\") > 5)) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple CountDistinct Aggregations\n",
    "\n",
    "This example shows computing multiple distinct counts in a single aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setJobDescription(\"Multiple countDistinct aggregations\")\n",
    "dfCounts = df.select(\n",
    "    F.countDistinct(F.col(\"ss_customer_sk\")).alias(\"distinct_customers\"),\n",
    "    F.countDistinct(F.col(\"ss_item_sk\")).alias(\"distinct_items\"),\n",
    "    F.countDistinct(F.col(\"ss_store_sk\")).alias(\"distinct_stores\"),\n",
    "    F.countDistinct(F.col(\"ss_promo_sk\")).alias(\"distinct_promotions\")\n",
    ")\n",
    "dfCounts.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
